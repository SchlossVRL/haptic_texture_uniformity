---
title: "Haptic Texture Uniformity Ratings"
format: html
editor: visual
---

### Load packages

```{r}
#rm(list = ls())

# List of packages
packages <- c("corrr", "ggplot2", "lme4", "lmerTest", "FactoMineR", "grid","png","cowplot","magick", "ggimage","dplyr","tidyverse","knitr", "jsonlite", "skimr")


```

```{r}

install_and_load_packages <- function(packages) {
  for(package in packages) {
    if(!require(package, character.only = TRUE)) {
      install.packages(package, dependencies = TRUE)
      library(package, character.only = TRUE)
    }
  }
}

# Call the function with the list of packages
install_and_load_packages(packages)

```

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

## Set working directories

```{r}
current_dir<- getwd()

data_dir <-paste0(current_dir,'/../data/')
data_files <-list.files(path = data_dir, full.names = TRUE, pattern="*.csv")
print(paste0('we have data from ',length(data_files),' participants'))



```

## Load in data and filter our texture ids

```{r}

df_list <- lapply(data_files, read.csv)

experiment_data_raw<- bind_rows(df_list)
ratings_trials<- experiment_data_raw %>% filter(trial_type== 'survey-text')


#correct recorded errors
# participant 20 texture d15 response should be 2.5



#### For now leaving as is and just removing NAs, but fix later once code works

ratings_trials_exp <- ratings_trials %>%
  mutate(response = map_chr(response, ~ fromJSON(.x)$Q0)) %>%
  mutate(response = as.numeric(response)) %>%
  filter(!is.na(response))  # Optionally remove rows with NA responses


# extract_texture_id<-function(filename){
#   f<- str_split_i(filename,"/",2)
#   f<- str_split_i(f,".png",1)
#   return(f)
#   
# }

#ratings_trials_exp$texture <- lapply(ratings_trials_exp$texture_id,extract_texture_id)


# Create condition_num column based on texture list
ratings_trials_exp <- ratings_trials_exp %>%
  mutate(condition_num = case_when(
    list_name == "textures_list1" ~ 1,
    list_name == "textures_list2" ~ 2,
    list_name == "textures_list3" ~ 3
  ))

# unique_sona_ids <- unique(ratings_trials_exp$sona_id)
# cat(paste(unique_sona_ids, collapse = ", "))

# get rid of error entry
ratings_trials_exp <- ratings_trials_exp %>%
  filter(response <= 7)

```

```{r}
t<-ratings_trials_exp %>%
  group_by(condition_num) %>%
  summarise(unique_subjects = n_distinct(subject_id))

texture_groups <- ratings_trials_exp %>% 
  select(texture, condition_num) %>% 
  distinct() %>% 
  arrange(condition_num)
```

```{r}
#later for testing split half correlations
colnames(t) <- c("condition_num", "unique_subjects")
t2 <- as.data.frame(t)
t2 <- t2 %>% 
  mutate(
    group = condition_num
  )
```

```{r}

bootstrap_ratings<- function(ratings_df, sample_size){
  
  group1ids<- ratings_df %>%
  group_by(condition_num) %>%
  sample_n(sample_size,replace = TRUE)%>%select(subject_id)
  
  group2ids <- ratings_df %>%
  # filter(!subject_id %in% group1ids$subject_id) %>%
  group_by(condition_num) %>%
  sample_n(sample_size, replace = TRUE) %>%
  select(subject_id)
  
  group1df<-ratings_df%>%filter(subject_id%in%group1ids$subject_id)
  group2df<-ratings_df%>%filter(subject_id%in%group2ids$subject_id)
  
  group1mat<- group1df%>% group_by(texture) %>%summarise(mean_rating = mean(response),.groups='keep')%>%
  pivot_wider(names_from = texture, values_from = mean_rating)
  group2mat<- group2df%>% group_by(texture) %>%summarise(mean_rating = mean(response),.groups='keep')%>%
  pivot_wider(names_from = texture, values_from = mean_rating)
  
  common_columns <- intersect(colnames(group1mat), colnames(group2mat))  # Find common columns
  group1mat <- group1mat[, sort(common_columns)]
  group2mat <- group2mat[, sort(common_columns)]
  
  rowwise_correlations <- mapply(function(row1, row2) {
    cor(row1, row2, use = "pairwise.complete.obs")
  }, as.data.frame(t(group1mat)), as.data.frame(t(group2mat)))
  
  # Compute mean correlation
  mean_correlation <- mean(rowwise_correlations, na.rm = TRUE)
  
  return(mean_correlation)

  
}



```

##By group

```{r}

# Initialize lists to store results
iters <- numeric()
sample_sizes <- numeric() # note the sample sizes are the number of samples taken in bootstrapping sequence, so for that stability the number of participants should be doubled.
mean_rs <- numeric()
groups <- character()  # Store group information

# Specify the grouping column
group_column <- "condition_num"  

# Loop over each group
for (group in unique(ratings_trials_exp[[group_column]])) {
  group_data <- ratings_trials_exp %>%
    filter(!!sym(group_column) == group)  # Filter data for the group
  
  for (iter in 1:1000) {
    for (sample_size in seq(5, 50, by = 5)) {
      # Run bootstrap_ratings for this group and sample size
      result <- bootstrap_ratings(group_data, sample_size)
      
      # Store the results
      mean_rs <- c(mean_rs, result)
      iters <- c(iters, iter)
      sample_sizes <- c(sample_sizes, sample_size)
      groups <- c(groups, group)
    }
  }
}

# Combine results into a data frame
bootstrap_df <- data.frame(iters, sample_sizes, mean_rs, groups)
colnames(bootstrap_df) <- c('iteration', 'sample_size', 'mean_reliability', 'group')

# Summarize results by group and sample size
summary_results <- bootstrap_df %>%
  group_by(group, sample_size) %>%
  summarise(
    se_reliability = sd(mean_reliability, na.rm = TRUE) / sqrt(n()),
    mean_reliability = mean(mean_reliability, na.rm = TRUE),
    .groups = 'drop'
  )

# Create ggplot grouped by group
ggplot(summary_results, aes(x = sample_size, y = mean_reliability, color = group)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_reliability - se_reliability,
                  ymax = mean_reliability + se_reliability,
                  fill = group),
              alpha = 0.1) +
  labs(
    title = "Mean Reliability Across Sample Sizes by Group",
    x = "Sample Size",
    y = "Mean Reliability"
  ) +
  theme_minimal() +
  ylim(0, 1)


```

```{r}
#compute correlations for current sample sizes in each group
compute_correlations <- function(ratings_df, condition) {
  correlation_results <- vector("list", 1000)
  
  for (i in 1:1000) {
    # Randomly split subjects into two groups within the given condition
    group1ids <- ratings_df %>%
      filter(condition_num == condition) %>%
      sample_n(t2 %>% filter(condition_num == condition) %>% pull(unique_subjects) / 2, replace = FALSE) %>%
      select(subject_id)
    
    group2ids <- ratings_df %>%
      filter(condition_num == condition) %>%
      filter(!subject_id %in% group1ids$subject_id) %>%
      sample_n(t2 %>% filter(condition_num == condition) %>% pull(unique_subjects) / 2, replace = FALSE) %>%
      select(subject_id)
    
    # Create data matrices for both groups
    group1df <- ratings_df %>% filter(subject_id %in% group1ids$subject_id)
    group2df <- ratings_df %>% filter(subject_id %in% group2ids$subject_id)
    
    group1mat <- group1df %>%
      group_by(texture) %>%
      summarise(mean_rating = mean(response), .groups = 'keep') %>%
      pivot_wider(names_from = texture, values_from = mean_rating)
    
    group2mat <- group2df %>%
      group_by(texture) %>%
      summarise(mean_rating = mean(response), .groups = 'keep') %>%
      pivot_wider(names_from = texture, values_from = mean_rating)
    
    common_columns <- intersect(colnames(group1mat), colnames(group2mat))
    group1mat <- group1mat[, sort(common_columns)]
    group2mat <- group2mat[, sort(common_columns)]
    
    # Compute correlations
    rowwise_correlations <- mapply(
      function(row1, row2) cor(row1, row2, use = "pairwise.complete.obs"),
      as.data.frame(t(group1mat)), as.data.frame(t(group2mat))
    )
    
    # Store results
    correlation_results[[i]] <- data.frame(
      iteration = i,
      condition_num = condition,  # Store the condition
      correlation = rowwise_correlations
    )
  }
  
  return(bind_rows(correlation_results))  # Combine all iterations into a single dataframe
}

all_conditions <- unique(ratings_trials_exp$condition_num)

all_correlation_results <- bind_rows(lapply(all_conditions, function(cond) {
  compute_correlations(ratings_trials_exp, cond)
}))

```

```{r}
#plotting
#first add "group" column to all_correlation_results based on condition_num column to match up with summary_resutls from bootstrapping

all_correlation_results <- all_correlation_results %>% 
  mutate(
    group = as.character(condition_num)
  )
#join with summary_results for plotting
summary_results_join <- left_join(summary_results, all_correlation_results, by = "group")

#add group participant counts from t2
t2 <- t2 %>% 
  mutate(
    group = as.character(group)
  )

summary_results_join <- left_join(summary_results_join, t2, by = "group")

#add average correlation per group based on split half correlations

summary_results_join <- summary_results_join %>% 
  group_by(group) %>% 
  mutate(
    split_half_corr = mean(correlation)) %>% 
  ungroup()

# Plot distribution of correlations for each group
ggplot(summary_results_join, aes(x = correlation)) +
  geom_density(fill = "blue", alpha = 0.3) +
  facet_wrap(~ group, scales = "fixed") +
  labs(title = "Distribution of Correlations Over 1000 Iterations",
       x = "Correlation",
       y = "Density") +
  theme_minimal()

#combined plot

#use summary results from boostrapping to compare
#create vertical line each group at current sample size
#add point for correlation for each group at current sample size

# Create ggplot grouped by group
ggplot(summary_results_join, aes(x = sample_size, y = mean_reliability, color = group)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_reliability - se_reliability,
                  ymax = mean_reliability + se_reliability,
                  fill = group),
              alpha = 0.1) +
    geom_vline(
    data = summary_results_join,
    aes(xintercept = unique_subjects/2, color = group, group = group),
    linetype = "dashed"
  ) +
  geom_point(
    data = summary_results_join,
    aes(x = unique_subjects/2, y = split_half_corr, color = group, group = group)
  ) +
  labs(
    title = "Mean Reliability Across Sample Sizes by Group",
    x = "Sample Size",
    y = "Mean Reliability"
  ) +
  theme_minimal() +
  ylim(0, 1)

```

```{r}
#All together bootstrpping (if desired) ---- Not by group 
iters<-numeric()
sample_sizes<-numeric() # note the sample sizes are the number of samples taken in bootstrapping sequence, so for that stability the number of participants should be doubled. 
mean_rs<-numeric()
#correlation_dfs<-list()
for(iter in 1:1000){
for(sample_size in seq(5, 50, by = 5)){
  result <- bootstrap_ratings(ratings_trials_exp, sample_size)
  mean_rs <- c(mean_rs, result)
  #correlation_dfs[[length(correlation_dfs) + 1]] <- result$correlation_df
  iters <- c(iters, iter)
  sample_sizes <- c(sample_sizes, sample_size)

}
}


bootstrap_df <- data.frame(cbind( iters,sample_sizes,mean_rs
))
colnames(bootstrap_df)<- c('iteration','sample_size','mean_reliability')


# Summarize data to calculate mean and standard error for each sample_size
summary_results <-bootstrap_df %>%
    group_by(sample_size) %>%
    summarise(
        se_reliability = sd(mean_reliability, na.rm = TRUE) / sqrt(n()),
        mean_reliability = mean(mean_reliability, na.rm = TRUE),
       
    )

# Create ggplots
ggplot(summary_results, aes(x = sample_size, y = mean_reliability)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = mean_reliability - se_reliability,
                  ymax = mean_reliability + se_reliability),
              alpha = 0.1, fill = "red") +
  labs(
    title = "Mean Reliability Across Sample Sizes",
    x = "Sample Size",
    y = "Mean Reliability"
  ) +
  theme_minimal()
```

##Determining Subset

```{r}
#Confidence Intervals
ci_data <- ratings_trials_exp %>%
  group_by(texture) %>%
  summarise(
    mean_rating = mean(response, na.rm = TRUE),
    ci_lower = mean(response, na.rm = TRUE) - qt(0.975, df = n() - 1) * sd(response, na.rm = TRUE) / sqrt(n()),
    ci_upper = mean(response, na.rm = TRUE) + qt(0.975, df = n() - 1) * sd(response, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"  # Drop grouping after summarize
  )

# View the confidence intervals
print(ci_data)

ci_data <- ci_data %>% 
  arrange(desc(mean_rating))

ci_data
ci_data_reduced <- ci_data %>%
  filter(mean_rating >= 4) %>% 
  select(texture)
write_csv(ci_data_reduced, "ci_data.csv")
```

```{r}
#Error
error_data <- ratings_trials_exp %>%
  group_by(texture) %>%
  summarise(
    mean_rating = mean(response, na.rm = TRUE),
    se = sd(response, na.rm = TRUE) / sqrt(n()),  # Standard error of the mean
    error_lower = mean_rating - se,               # Lower bound for error bars
    error_upper = mean_rating + se,               # Upper bound for error bars
    .groups = "drop"  # Drop grouping after summarize
  )

# View the error bars
print(error_data)

# Arrange data by mean rating in descending order
error_data <- error_data %>% 
  arrange(desc(mean_rating))

error_data
error_data_reduced <- error_data %>%
  filter(error_lower >= 4) %>% 
  select(texture)
write_csv(error_data_reduced, "error_data.csv")
```

```{r}
# ci
ggplot(ci_data, aes(x = reorder(texture, -mean_rating), y = mean_rating)) +
  geom_point(size = 3, color = "skyblue") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.5, color = "darkblue") +
  labs(
    title = "Average Uniformity Ratings by Texture - CI bars",
    x = "Image",
    y = "Average Uniformity Rating"
  ) +
  theme_minimal() +
  #scale_y_continuous(limits = c(1, 7), breaks = seq(1, 7, 1)) +
  theme(
    #axis.text.x = element_blank(),  # Remove x-axis text labels
    #axis.ticks.x = element_blank()   # Remove x-axis ticks
  )+
  #ylim(1, 7)
  coord_cartesian(ylim = c(1, 7))
```

```{r}
ggplot(error_data, aes(x = reorder(texture, -mean_rating), y = mean_rating)) +
  geom_errorbar(aes(ymin = error_lower, ymax = error_upper), width = .5, color = "darkblue") +
  geom_point(size = 3, color = "skyblue") +
  labs(
    title = "Average Uniformity Ratings by Texture - Error bars",
    x = "Image",
    y = "Average Uniformity Rating"
  ) +
  theme_minimal() +
  #scale_y_continuous(limits = c(1, 7), breaks = seq(1, 7, 1)) +
  theme(
    #axis.text.x = element_blank(),  # Remove x-axis text labels
    #axis.ticks.x = element_blank()   # Remove x-axis ticks
  ) +
  #ylim(1, 7)
  coord_cartesian(ylim = c(1, 7))
```
